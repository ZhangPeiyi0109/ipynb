{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.tsv', sep='\\t')\n",
    "valid_data = pd.read_csv('data/dev.tsv', sep='\\t')\n",
    "test_data = pd.read_csv('data/test.tsv', sep='\\t') \n",
    "x_train, y_train = train_data.text_a.values, train_data.label.values # 训练集\n",
    "x_valid, y_valid = valid_data.text_a.values, valid_data.label.values # 验证集\n",
    "x_test, y_test = test_data.text_a.values, test_data.label.values # 测试集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>房间太小。其他的都一般。。。。。。。。。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.接电源没有几分钟,电源适配器热的不行. 2.摄像头用不起来. 3.机盖的钢琴漆，手不能摸...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>今天才知道这书还有第6卷,真有点郁闷:为什么同一套书有两种版本呢?当当网是不是该跟出版社商量...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9141</th>\n",
       "      <td>1</td>\n",
       "      <td>看过该书，感觉中医暂时不会消亡，尚有一、二十株老树活着，还有毛以林、黄煌、刘力红等一批有一定...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9142</th>\n",
       "      <td>0</td>\n",
       "      <td>这本书没读到底，不是特别喜欢。完全可以用序中的评价来表达我的感受：可以包容，却不想实践。除了...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9143</th>\n",
       "      <td>1</td>\n",
       "      <td>虽是观景房,不过我住的楼层太低(19楼)看不到江景,但地点很好,离轻轨临江门站和较场口站(起...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9144</th>\n",
       "      <td>1</td>\n",
       "      <td>性价比不错，交通方便。行政楼层感觉很好，只是早上8点楼上装修，好吵。 中餐厅档次太低，虽然便...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9145</th>\n",
       "      <td>0</td>\n",
       "      <td>跟心灵鸡汤没什么本质区别嘛，至少我不喜欢这样读经典，把经典都解读成这样有点去中国化的味道了</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9146 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                             text_a\n",
       "0         1  选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全...\n",
       "1         1  15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很...\n",
       "2         0                               房间太小。其他的都一般。。。。。。。。。\n",
       "3         0  1.接电源没有几分钟,电源适配器热的不行. 2.摄像头用不起来. 3.机盖的钢琴漆，手不能摸...\n",
       "4         1  今天才知道这书还有第6卷,真有点郁闷:为什么同一套书有两种版本呢?当当网是不是该跟出版社商量...\n",
       "...     ...                                                ...\n",
       "9141      1  看过该书，感觉中医暂时不会消亡，尚有一、二十株老树活着，还有毛以林、黄煌、刘力红等一批有一定...\n",
       "9142      0  这本书没读到底，不是特别喜欢。完全可以用序中的评价来表达我的感受：可以包容，却不想实践。除了...\n",
       "9143      1  虽是观景房,不过我住的楼层太低(19楼)看不到江景,但地点很好,离轻轨临江门站和较场口站(起...\n",
       "9144      1  性价比不错，交通方便。行政楼层感觉很好，只是早上8点楼上装修，好吵。 中餐厅档次太低，虽然便...\n",
       "9145      0      跟心灵鸡汤没什么本质区别嘛，至少我不喜欢这样读经典，把经典都解读成这样有点去中国化的味道了\n",
       "\n",
       "[9146 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般',\n",
       "        '15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很美观，做工也相当不错',\n",
       "        '房间太小。其他的都一般。。。。。。。。。', ...,\n",
       "        '虽是观景房,不过我住的楼层太低(19楼)看不到江景,但地点很好,离轻轨临江门站和较场口站(起点)很近,解放碑就在附近(大约100多公尺吧)!',\n",
       "        '性价比不错，交通方便。行政楼层感觉很好，只是早上8点楼上装修，好吵。 中餐厅档次太低，虽然便宜，但是和酒店档次不相配。',\n",
       "        '跟心灵鸡汤没什么本质区别嘛，至少我不喜欢这样读经典，把经典都解读成这样有点去中国化的味道了'], dtype=object),\n",
       " array([1, 1, 0, ..., 1, 1, 0]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.985 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "cut_docs = train_data.text_a.apply(lambda x: jieba.cut(x)).values\n",
    "for doc in cut_docs:\n",
    "    for word in doc:\n",
    "        if word.strip():\n",
    "            vocab.add(word.strip())\n",
    "\n",
    "# 将词表写入本地vocab.txt文件\n",
    "with open('data/vocab.txt', 'w') as file:\n",
    "    for word in  vocab:\n",
    "        file.write(word)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    embedding_dim = 300 # 词向量维度\n",
    "    max_seq_len = 200 # 文章最大词数\n",
    "    vocab_file = 'data/vocab.txt' # 词汇表文件路径\n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        # 初始化词和id的映射词典，预留0给padding字符，1给词表中未见过的词\n",
    "        token2idx = {\"[PAD]\": 0, \"[UNK]\": 1} # {word：id}\n",
    "        with open(config.vocab_file, 'r') as reader:\n",
    "            for index, line in enumerate(reader):\n",
    "                token = line.strip()\n",
    "                token2idx[token] = index+2\n",
    "                \n",
    "        self.token2idx = token2idx\n",
    "        \n",
    "    def transform(self, text_list):\n",
    "        # 文本分词，并将词转换成相应的id, 最后不同长度的文本padding长统一长度，后面补0\n",
    "        idx_list = [[self.token2idx.get(word.strip(), self.token2idx['[UNK]']) for word in jieba.cut(text)] for text in text_list]\n",
    "        idx_padding = pad_sequences(idx_list, self.config.max_seq_len, padding='post')\n",
    "        \n",
    "        return idx_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[28574, 26383,  4176, 29924, 32111, 12032,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [14935, 22694, 12032, 22356,   697, 16538, 26494, 12032,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = Preprocessor(config)\n",
    "preprocessor.transform(['性价比不错，交通方便。', '房间太小。其他的都一般。'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.preprocessor = Preprocessor(config)\n",
    "        self.class_name = {0: '负面', 1: '正面'}\n",
    "    \n",
    "    def build_model(self):\n",
    "        # 模型架构搭建\n",
    "        idx_input = tf.keras.layers.Input((self.config.max_seq_len,))\n",
    "        input_embedding = tf.keras.layers.Embedding(len(self.preprocessor.token2idx),\n",
    "                    self.config.embedding_dim,\n",
    "                    input_length=self.config.max_seq_len,\n",
    "                    mask_zero=True)(idx_input)\n",
    "        convs = []\n",
    "        for kernel_size in [3, 4, 5]:\n",
    "            c = tf.keras.layers.Conv1D(128, kernel_size, activation='relu')(input_embedding)\n",
    "            c = tf.keras.layers.GlobalMaxPooling1D()(c)\n",
    "            convs.append(c)\n",
    "        fea_cnn = tf.keras.layers.Concatenate()(convs)\n",
    "        fea_cnn_dropout = tf.keras.layers.Dropout(rate=0.4)(fea_cnn)\n",
    "        \n",
    "        fea_dense = tf.keras.layers.Dense(128, activation='relu')(fea_cnn_dropout)\n",
    "        output = tf.keras.layers.Dense(2, activation='softmax')(fea_dense)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=idx_input, outputs=output)\n",
    "        model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        self.model = model\n",
    "    \n",
    "    def fit(self, x_train, y_train, x_valid=None, y_valid=None, epochs=5, batch_size=128, callbacks=None, **kwargs):\n",
    "        # 训练\n",
    "        self.build_model()\n",
    "        \n",
    "        x_train = self.preprocessor.transform(x_train)\n",
    "        valid_data = None\n",
    "        if x_valid is not None and y_valid is not None:\n",
    "            x_valid = self.preprocessor.transform(x_valid)\n",
    "            valid_data = (x_valid, y_valid)\n",
    "\n",
    "        self.model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            validation_data= valid_data,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        # 评估\n",
    "        x_test = self.preprocessor.transform(x_test)\n",
    "        y_pred_probs = self.model.predict(x_test)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=-1)\n",
    "        result = classification_report(y_test, y_pred, target_names=['负面', '正面'])\n",
    "        print(result)\n",
    "        \n",
    "        \n",
    "    def single_predict(self, text):\n",
    "        # 预测\n",
    "        input_idx = self.preprocessor.transform([text])\n",
    "        predict_prob = self.model.predict(input_idx)[0]\n",
    "        predict_label_id = np.argmax(predict_prob)\n",
    "        \n",
    "        predict_label_name = self.class_name[predict_label_id]\n",
    "        predict_label_prob = predict_prob[predict_label_id]\n",
    "        \n",
    "        return predict_label_name, predict_label_prob\n",
    "    \n",
    "    def load_model(self, ckpt_file):\n",
    "        self.build_model()\n",
    "        self.model.load_weights(ckpt_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 300)     10527900    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 198, 128)     115328      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 197, 128)     153728      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 196, 128)     192128      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 128)          0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 384)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 384)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          49280       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            258         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 11,038,622\n",
      "Trainable params: 11,038,622\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "72/72 [==============================] - 9s 118ms/step - loss: 0.5293 - accuracy: 0.7253 - val_loss: 0.3638 - val_accuracy: 0.8475\n",
      "Epoch 2/50\n",
      "72/72 [==============================] - 8s 106ms/step - loss: 0.2018 - accuracy: 0.9260 - val_loss: 0.2658 - val_accuracy: 0.9050\n",
      "Epoch 3/50\n",
      "72/72 [==============================] - 7s 97ms/step - loss: 0.0645 - accuracy: 0.9823 - val_loss: 0.3003 - val_accuracy: 0.9025\n",
      "Epoch 4/50\n",
      "72/72 [==============================] - 7s 97ms/step - loss: 0.0381 - accuracy: 0.9907 - val_loss: 0.3262 - val_accuracy: 0.9033\n",
      "Epoch 5/50\n",
      "72/72 [==============================] - 7s 97ms/step - loss: 0.0265 - accuracy: 0.9942 - val_loss: 0.2976 - val_accuracy: 0.9125\n",
      "Epoch 6/50\n",
      "72/72 [==============================] - 7s 98ms/step - loss: 0.0226 - accuracy: 0.9949 - val_loss: 0.3010 - val_accuracy: 0.9117\n",
      "Epoch 7/50\n",
      "72/72 [==============================] - 7s 97ms/step - loss: 0.0189 - accuracy: 0.9961 - val_loss: 0.3269 - val_accuracy: 0.9058\n",
      "Epoch 8/50\n",
      "72/72 [==============================] - 7s 97ms/step - loss: 0.0156 - accuracy: 0.9965 - val_loss: 0.3364 - val_accuracy: 0.8958\n"
     ]
    }
   ],
   "source": [
    "# 定义early stop早停回调函数\n",
    "patience = 6\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "\n",
    "# 定义checkpoint回调函数\n",
    "checkpoint_prefix = './checkpoints/textcnn_imdb_ckpt'\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)\n",
    "\n",
    "# 初始化模型类，启动训练\n",
    "textcnn = TextCNN(config)\n",
    "textcnn.fit(x_train, y_train, x_valid, y_valid, epochs=50, callbacks=[early_stop, checkpoint_callback]) # 训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          负面       0.90      0.92      0.91       592\n",
      "          正面       0.92      0.90      0.91       608\n",
      "\n",
      "    accuracy                           0.91      1200\n",
      "   macro avg       0.91      0.91      0.91      1200\n",
      "weighted avg       0.91      0.91      0.91      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textcnn.evaluate(x_test, y_test) # 测试集评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     10527900    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 198, 128)     115328      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 197, 128)     153728      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 196, 128)     192128      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 128)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 384)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          49280       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            258         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 11,038,622\n",
      "Trainable params: 11,038,622\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "textcnn = TextCNN(config)\n",
    "textcnn.load_model(checkpoint_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('正面', 0.9999037)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textcnn.single_predict(\"外观很漂亮，出人意料地漂亮，做工非常好\") # 单句预测\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('负面', 0.999212)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textcnn.single_predict(\"书的内容没什么好说的，主要是纸张、印刷太差，所用的纸非常粗糙比一般的盗版书还要差，裁的也不好。\") # 单句预测"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8dfe1a2bdf305088d2dafae6ec29e39d768339b76b5be5dab4d225f4fc5457b7"
  },
  "kernelspec": {
   "display_name": "Python 3.6  ('tensorflow2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}